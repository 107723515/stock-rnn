{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "random.seed(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 200,\n",
       " 'embedding_size': 8,\n",
       " 'init_epoch': 5,\n",
       " 'init_learning_rate': 0.05,\n",
       " 'input_size': 5,\n",
       " 'keep_prob': 0.8,\n",
       " 'learning_rate_decay': 0.99,\n",
       " 'lstm_size': 32,\n",
       " 'max_epoch': 500,\n",
       " 'num_layers': 1,\n",
       " 'num_steps': 30,\n",
       " 'sector_size': -1,\n",
       " 'stock_symbol_size': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNNConfig():\n",
    "    __data__ = dict(\n",
    "        input_size=5,\n",
    "        num_steps=30,\n",
    "        lstm_size=32,\n",
    "\n",
    "        num_layers=1,\n",
    "        keep_prob=0.8,\n",
    "        batch_size = 200,\n",
    "        init_learning_rate = 0.05,\n",
    "        learning_rate_decay = 0.99,\n",
    "        init_epoch = 5,\n",
    "        max_epoch = 500,\n",
    "\n",
    "        embedding_size = 8,\n",
    "        sector_size = None,\n",
    "        stock_symbol_size = None,\n",
    "    )\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__data__\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.to_dict())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_dict())\n",
    "    \n",
    "    def __setattr__(self, name, val):\n",
    "        self.__data__[name] = val\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return self.__data__[name]\n",
    "\n",
    "config = RNNConfig()\n",
    "config.sector_size = -1\n",
    "config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05, 0.05, 0.05, 0.05, 0.05, 0.0495, 0.049005, 0.04851495, 0.0480298005, 0.047549502495, 0.04707400747005, 0.0466032673953495, 0.046137234721396005, 0.045675862374182044, 0.04521910375044022, 0.04476691271293582, 0.04431924358580647, 0.04387605114994839, 0.043437290638448915, 0.043002917732064425]\n",
      "middle: 0.00421917826632\n",
      "[0.00041814088372523514, 0.0004139594748879828, 0.00040981988013910294, 0.0004057216813377119, 0.00040166446452433483, 0.0003976478198790915, 0.00039367134168030054, 0.0003897346282634975, 0.00038583728198086253, 0.0003819789091610539, 0.00037815912006944336, 0.00037437752886874894, 0.0003706337535800614, 0.00036692741604426086, 0.0003632581418838182, 0.00035962556046498, 0.00035602930486033023, 0.00035246901181172695, 0.00034894432169360964, 0.00034545487847667354]\n"
     ]
    }
   ],
   "source": [
    "learning_rates_to_use = [\n",
    "    config.init_learning_rate * (\n",
    "        config.learning_rate_decay ** max(float(i + 1 - config.init_epoch), 0.0)\n",
    "    ) for i in range(config.max_epoch)]\n",
    "print learning_rates_to_use[:20]\n",
    "print 'middle:', learning_rates_to_use[len(learning_rates_to_use) // 2]\n",
    "print learning_rates_to_use[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StockDataSet(object):\n",
    "    def __init__(self, \n",
    "                 stock_sym, \n",
    "                 stock_sector,\n",
    "                 config,\n",
    "                 test_ratio=0.1,\n",
    "                 normalized=True,\n",
    "                 close_price_only=True,\n",
    "                 **kwargs):\n",
    "        \n",
    "        self.stock_sym = stock_sym\n",
    "        self.stock_sector = stock_sector\n",
    "\n",
    "        self.input_size = config.input_size\n",
    "        self.num_steps = config.num_steps\n",
    "        self.test_ratio = test_ratio\n",
    "        self.close_price_only = close_price_only\n",
    "        self.normalized = normalized\n",
    "        \n",
    "        # Read csv file\n",
    "        raw_df = pd.read_csv(\"_data/%s.csv\" % stock_sym)\n",
    "        \n",
    "        # Merge into one sequence\n",
    "        if close_price_only:\n",
    "            self.raw_seq = raw_df['Close'].tolist()\n",
    "        else:\n",
    "            self.raw_seq = [price for tup in raw_df[['Open', 'Close']].values for price in tup]\n",
    "        \n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self._prepare_data(self.raw_seq)\n",
    "\n",
    "    def _prepare_data(self, seq):\n",
    "        # split into items of input_size\n",
    "        seq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size]) \n",
    "               for i in range(len(seq) // self.input_size)]\n",
    "\n",
    "        if self.normalized:\n",
    "            seq = [seq[0] / seq[0][0] - 1.0] + [\n",
    "                curr / seq[i - 1][-1] - 1.0 for i, curr in enumerate(seq[1:])]\n",
    "        \n",
    "        # split into groups of num_steps\n",
    "        X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "        y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.test_ratio))\n",
    "        train_X, test_X = X[:train_size], X[train_size:]\n",
    "        train_y, test_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, test_X, test_y\n",
    "    \n",
    "    def generate_one_epoch(self, batch_size):\n",
    "        num_batches = int(len(self.train_X)) // batch_size\n",
    "        if batch_size * num_batches < len(self.train_X):\n",
    "            num_batches += 1\n",
    "        \n",
    "        batch_indices = range(num_batches)\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = self.train_X[j * batch_size: (j+1) * batch_size]\n",
    "            batch_y = self.train_y[j * batch_size: (j+1) * batch_size]\n",
    "            assert set(map(len, batch_X)) == {self.num_steps}\n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    381\n",
      "True     124\n",
      "dtype: int64\n",
      "(124, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>name</th>\n",
       "      <th>sector</th>\n",
       "      <th>price</th>\n",
       "      <th>dividend yield</th>\n",
       "      <th>price/earnings</th>\n",
       "      <th>earnings/share</th>\n",
       "      <th>book value</th>\n",
       "      <th>52 week low</th>\n",
       "      <th>52 week high</th>\n",
       "      <th>market cap</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>price/sales</th>\n",
       "      <th>price/book</th>\n",
       "      <th>sec filings</th>\n",
       "      <th>file_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATVI</td>\n",
       "      <td>Activision Blizzard</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>48.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>37.55</td>\n",
       "      <td>1.28</td>\n",
       "      <td>12.23</td>\n",
       "      <td>30.37</td>\n",
       "      <td>48.36</td>\n",
       "      <td>36.13</td>\n",
       "      <td>2.14000</td>\n",
       "      <td>5.44</td>\n",
       "      <td>3.91</td>\n",
       "      <td>http://www.sec.gov/cgi-bin/browse-edgar?action...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>Adobe Systems Inc</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>119.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>51.72</td>\n",
       "      <td>2.32</td>\n",
       "      <td>15.02</td>\n",
       "      <td>83.25</td>\n",
       "      <td>120.69</td>\n",
       "      <td>59.28</td>\n",
       "      <td>1.82000</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.00</td>\n",
       "      <td>http://www.sec.gov/cgi-bin/browse-edgar?action...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AKAM</td>\n",
       "      <td>Akamai Technologies Inc</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>63.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.36</td>\n",
       "      <td>1.79</td>\n",
       "      <td>18.61</td>\n",
       "      <td>47.80</td>\n",
       "      <td>71.64</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.70798</td>\n",
       "      <td>4.67</td>\n",
       "      <td>3.39</td>\n",
       "      <td>http://www.sec.gov/cgi-bin/browse-edgar?action...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ALXN</td>\n",
       "      <td>Alexion Pharmaceuticals</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>129.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.40</td>\n",
       "      <td>1.76</td>\n",
       "      <td>38.81</td>\n",
       "      <td>109.12</td>\n",
       "      <td>162.00</td>\n",
       "      <td>29.02</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>9.71</td>\n",
       "      <td>3.44</td>\n",
       "      <td>http://www.sec.gov/cgi-bin/browse-edgar?action...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc Class A</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>851.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.53</td>\n",
       "      <td>27.88</td>\n",
       "      <td>201.12</td>\n",
       "      <td>672.66</td>\n",
       "      <td>867.00</td>\n",
       "      <td>588.50</td>\n",
       "      <td>29.86000</td>\n",
       "      <td>6.49</td>\n",
       "      <td>4.21</td>\n",
       "      <td>http://www.sec.gov/cgi-bin/browse-edgar?action...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol                     name                  sector   price  \\\n",
       "4    ATVI      Activision Blizzard  Information Technology   48.06   \n",
       "6    ADBE        Adobe Systems Inc  Information Technology  119.98   \n",
       "14   AKAM  Akamai Technologies Inc  Information Technology   63.30   \n",
       "17   ALXN  Alexion Pharmaceuticals             Health Care  129.18   \n",
       "23  GOOGL     Alphabet Inc Class A  Information Technology  851.15   \n",
       "\n",
       "    dividend yield  price/earnings  earnings/share  book value  52 week low  \\\n",
       "4             0.64           37.55            1.28       12.23        30.37   \n",
       "6             0.00           51.72            2.32       15.02        83.25   \n",
       "14             NaN           35.36            1.79       18.61        47.80   \n",
       "17             NaN           73.40            1.76       38.81       109.12   \n",
       "23             NaN           30.53           27.88      201.12       672.66   \n",
       "\n",
       "    52 week high  market cap    ebitda  price/sales  price/book  \\\n",
       "4          48.36       36.13   2.14000         5.44        3.91   \n",
       "6         120.69       59.28   1.82000        10.14        8.00   \n",
       "14         71.64       10.96   0.70798         4.67        3.39   \n",
       "17        162.00       29.02   1.17000         9.71        3.44   \n",
       "23        867.00      588.50  29.86000         6.49        4.21   \n",
       "\n",
       "                                          sec filings file_exists  \n",
       "4   http://www.sec.gov/cgi-bin/browse-edgar?action...        True  \n",
       "6   http://www.sec.gov/cgi-bin/browse-edgar?action...        True  \n",
       "14  http://www.sec.gov/cgi-bin/browse-edgar?action...        True  \n",
       "17  http://www.sec.gov/cgi-bin/browse-edgar?action...        True  \n",
       "23  http://www.sec.gov/cgi-bin/browse-edgar?action...        True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "SP500_symbols = pd.read_csv(\"_data/constituents-financials.csv\")\n",
    "SP500_symbols.rename(columns={col: col.lower() for col in SP500_symbols.columns}, inplace=True)\n",
    "SP500_symbols['file_exists'] = SP500_symbols['symbol'].map(lambda x: os.path.exists(\"_data/{}.csv\".format(x)))\n",
    "\n",
    "print SP500_symbols['file_exists'].value_counts()\n",
    "SP500_symbols = SP500_symbols[SP500_symbols['file_exists']]\n",
    "\n",
    "print SP500_symbols.shape\n",
    "SP500_symbols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "syms = SP500_symbols['symbol'].tolist()\n",
    "sym_to_sector = {row['symbol']: row['sector'] for _, row in SP500_symbols.iterrows()}\n",
    "stock_data_by_sym = {sym: StockDataSet(sym, sym_to_sector[sym], config, test_ratio=0.1) for sym in syms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Technology\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04555013, -0.08137981, -0.09448573, -0.08838159, -0.11025901],\n",
       "       [-0.08100061, -0.10406455, -0.2008013 , -0.186304  , -0.20059044],\n",
       "       [-0.12080947, -0.14634922, -0.14260186, -0.13242629, -0.11763861],\n",
       "       [ 0.02430099,  0.03201664,  0.03122521,  0.02449882,  0.02545501],\n",
       "       [ 0.04109767,  0.03384512,  0.01149953, -0.00032668,  0.02561252]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print stock_data_by_sym['GOOGL'].stock_sector\n",
    "stock_data_by_sym['GOOGL'].test_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sector_size': 8, 'embedding_size': 8, 'max_epoch': 500, 'keep_prob': 0.8, 'lstm_size': 32, 'num_steps': 30, 'stock_symbol_size': 124, 'batch_size': 200, 'init_learning_rate': 0.05, 'num_layers': 1, 'input_size': 5, 'init_epoch': 5, 'learning_rate_decay': 0.99}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>symbol</th>\n",
       "      <th>stock_symbol_index</th>\n",
       "      <th>sector</th>\n",
       "      <th>sector_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Activision Blizzard</td>\n",
       "      <td>ATVI</td>\n",
       "      <td>114</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adobe Systems Inc</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>28</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Akamai Technologies Inc</td>\n",
       "      <td>AKAM</td>\n",
       "      <td>93</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alexion Pharmaceuticals</td>\n",
       "      <td>ALXN</td>\n",
       "      <td>76</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Alphabet Inc Class A</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>36</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name symbol  stock_symbol_index  \\\n",
       "4       Activision Blizzard   ATVI                 114   \n",
       "6         Adobe Systems Inc   ADBE                  28   \n",
       "14  Akamai Technologies Inc   AKAM                  93   \n",
       "17  Alexion Pharmaceuticals   ALXN                  76   \n",
       "23     Alphabet Inc Class A  GOOGL                  36   \n",
       "\n",
       "                    sector  sector_index  \n",
       "4   Information Technology             0  \n",
       "6   Information Technology             0  \n",
       "14  Information Technology             0  \n",
       "17             Health Care             2  \n",
       "23  Information Technology             0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_sectors = SP500_symbols['sector'].value_counts().index.tolist()\n",
    "sector_to_index = {s: idx for idx, s in enumerate(uniq_sectors)}\n",
    "\n",
    "uniq_stock_symbols = SP500_symbols['symbol'].value_counts().index.tolist()\n",
    "symbol_to_index = {s: idx for idx, s in enumerate(uniq_stock_symbols)}\n",
    "\n",
    "config.sector_size = len(sector_to_index)\n",
    "config.stock_symbol_size = len(uniq_stock_symbols)\n",
    "print config.to_dict()\n",
    "\n",
    "SP500_symbols['stock_symbol_index'] = SP500_symbols['symbol'].map(lambda x: symbol_to_index[x])\n",
    "SP500_symbols['sector_index'] = SP500_symbols['sector'].map(lambda x: sector_to_index[x])\n",
    "SP500_symbols[['name', 'symbol', 'stock_symbol_index', 'sector', 'sector_index']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transpose - val.get_shape() = (?, 30, 32)\n",
      "After transpose - val.get_shape() = (30, ?, 32)\n",
      "(?, 8)\n",
      "last_state.get_shape() = (?, 32)\n",
      "last_state_with_embed.get_shape() = (?, 40)\n",
      "weight.get_shape() = (40, 5)\n",
      "bias.get_shape() = (5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "lstm_graph = tf.Graph()\n",
    "\n",
    "with lstm_graph.as_default():\n",
    "    \"\"\"\n",
    "    The model asks for three things:\n",
    "    - input: training data X\n",
    "    - targets: training label y\n",
    "    - learning_rate: \n",
    "    \"\"\"\n",
    "    learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "    stock_labels = tf.placeholder(tf.int32, [None,], name='stock_labels')  # mapped to an integer.\n",
    "\n",
    "    # Number of examples, number of input, dimension of each input\n",
    "    inputs = tf.placeholder(tf.float32, [None, config.num_steps, config.input_size], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.float32, [None, config.input_size], name=\"targets\")\n",
    "    \n",
    "    def _create_one_cell():\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(config.lstm_size, state_is_tuple=True)\n",
    "        if config.keep_prob < 1.0:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        return lstm_cell\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [_create_one_cell() for _ in range(config.num_layers)], \n",
    "        state_is_tuple=True\n",
    "    ) if config.num_layers > 1 else _create_one_cell()\n",
    "\n",
    "    val, state_ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, scope=\"lilian_rnn\")\n",
    "\n",
    "    print \"Before transpose - val.get_shape() =\", val.get_shape()\n",
    "\n",
    "    # Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size)\n",
    "    # After transpose, val.get_shape() = (num_steps, batch_size, lstm_size)\n",
    "    val = tf.transpose(val, [1, 0, 2])\n",
    "    print \"After transpose - val.get_shape() =\", val.get_shape()\n",
    "\n",
    "    embedding_matrix = tf.Variable(\n",
    "        tf.random_uniform([config.stock_symbol_size, config.embedding_size], -1.0, 1.0),\n",
    "        name=\"embedding_matrix\"\n",
    "    )\n",
    "    # industries_embed.shape = (batch_size, embedding_size)\n",
    "    stock_label_embeds = tf.nn.embedding_lookup(embedding_matrix, stock_labels)\n",
    "    print stock_label_embeds.get_shape()\n",
    "\n",
    "    with tf.name_scope(\"output_layer\"):\n",
    "        # last_state.get_shape() = (batch_size, lstm_size)\n",
    "        last_state = tf.gather(val, int(val.get_shape()[0]) - 1, name=\"last_LSTM_state\")\n",
    "        print \"last_state.get_shape() =\", last_state.get_shape()\n",
    "        \n",
    "        # After concat, last_state_with_embed.get_shape() = (batch_size, lstm_size + embedding_size)\n",
    "        last_state_with_embed = tf.concat([last_state, stock_label_embeds], axis=1, \n",
    "                                          name=\"last_LSTM_state_with_embed\")\n",
    "\n",
    "        weight = tf.Variable(\n",
    "            tf.truncated_normal([config.lstm_size + config.embedding_size, config.input_size]), \n",
    "            name=\"lilian_weights\"\n",
    "        )\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[config.input_size]), name=\"lilian_biases\")\n",
    "        print \"last_state_with_embed.get_shape() =\", last_state_with_embed.get_shape()\n",
    "        print \"weight.get_shape() =\", weight.get_shape()\n",
    "        print \"bias.get_shape() =\", bias.get_shape()\n",
    "        prediction = tf.matmul(last_state_with_embed, weight) + bias\n",
    "\n",
    "        tf.summary.histogram(\"last_LSTM_state\", last_state)\n",
    "        tf.summary.histogram(\"last_LSTM_state_with_embed\", last_state_with_embed)\n",
    "        tf.summary.histogram(\"weights\", weight)\n",
    "        tf.summary.histogram(\"biases\", bias)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        # loss = -tf.reduce_sum(targets * tf.log(tf.clip_by_value(prediction, 1e-10, 1.0)))\n",
    "        loss = tf.reduce_mean(tf.square(prediction - targets), name=\"loss_mse\")\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        minimize = optimizer.minimize(loss, name=\"loss_mse_adam_minimize\")\n",
    "        tf.summary.scalar(\"loss_mse\", loss)\n",
    "    \n",
    "    # operators you want to use after restoring the model\n",
    "    for op in [prediction, loss]:\n",
    "        tf.add_to_collection('ops_to_restore', op)\n",
    "\n",
    "    #mistakes = tf.not_equal(tf.argmax(targets, 1), tf.argmax(prediction, 1))\n",
    "    #error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged test data\n",
    "merged_test_X = []\n",
    "merged_test_y = []\n",
    "merged_stock_sector_indices = []\n",
    "merged_stock_symbol_indices = []\n",
    "\n",
    "for symbol, stock_dataset in stock_data_by_sym.iteritems():\n",
    "    stock_sector_index = sector_to_index[stock_dataset.stock_sector]\n",
    "    stock_symbol_index = symbol_to_index[stock_dataset.stock_sym]\n",
    "    \n",
    "    test_size = len(stock_dataset.test_X)\n",
    "    \n",
    "    merged_test_X += list(stock_dataset.test_X)\n",
    "    merged_test_y += list(stock_dataset.test_y)\n",
    "    merged_stock_sector_indices += [stock_sector_index] * test_size\n",
    "    merged_stock_symbol_indices += [stock_symbol_index] * test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13877, 30, 5)\n",
      "(13877, 5)\n",
      "(13877,)\n",
      "(13877,)\n"
     ]
    }
   ],
   "source": [
    "merged_test_X = np.array(merged_test_X)\n",
    "merged_test_y = np.array(merged_test_y)\n",
    "merged_stock_sector_indices = np.array(merged_stock_sector_indices)\n",
    "merged_stock_symbol_indices = np.array(merged_stock_symbol_indices)\n",
    "\n",
    "print merged_test_X.shape\n",
    "print merged_test_y.shape\n",
    "print merged_stock_sector_indices.shape\n",
    "print merged_stock_symbol_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def _mkdir_if_not_exist(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        print \"Create folder:\", dir_name\n",
    "        os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: All_lr0.05_lr_decay0.990_lstm32_step30_input5_batch200_epoch500_symbol_embed8\n",
      "Create folder: _logs/All_lr0.05_lr_decay0.990_lstm32_step30_input5_batch200_epoch500_symbol_embed8\n",
      "Create folder: _models/All_lr0.05_lr_decay0.990_lstm32_step30_input5_batch200_epoch500_symbol_embed8\n",
      "Epoch 0 [0.050000]: 0.00939097\n",
      "Predictions: [([0.035, 0.0236, 0.0169, 0.0221, 0.0165], [0.0161, 0.0, -0.0161, -0.0161, 0.0]), ([0.0798, 0.0516, 0.0316, 0.0545, 0.0588], [0.0727, 0.0909, 0.1091, 0.1091, 0.1273]), ([-0.0346, -0.0477, -0.0085, -0.0215, -0.0155], [0.0182, 0.0364, 0.0364, 0.0364, 0.0727]), ([0.0331, 0.0209, 0.0175, 0.0348, 0.0043], [0.0182, 0.0, -0.0182, 0.0, 0.0]), ([0.0216, 0.0468, 0.0031, 0.028, 0.0089], [0.0189, 0.0377, 0.0, 0.0189, 0.0377])]\n",
      "Epoch 5 [0.049500]: 0.331137\n",
      "Epoch 10 [0.047074]: 0.429627\n",
      "Epoch 15 [0.044767]: 0.199325\n",
      "Epoch 20 [0.042573]: 0.220103\n",
      "Epoch 25 [0.040486]: 0.141822\n",
      "Epoch 30 [0.038502]: 0.0862431\n",
      "Epoch 35 [0.036615]: 0.284868\n",
      "Epoch 40 [0.034821]: 0.0305238\n",
      "Epoch 45 [0.033114]: 0.14781\n",
      "Epoch 50 [0.031491]: 0.0396163\n",
      "Predictions: [([-0.1581, 0.0497, -0.3215, 0.1217, 0.0652], [0.0161, 0.0, -0.0161, -0.0161, 0.0]), ([0.0638, 0.0312, 0.0957, 0.0962, 0.0489], [0.0727, 0.0909, 0.1091, 0.1091, 0.1273]), ([-0.0002, -0.0312, 0.0254, 0.0323, -0.0179], [0.0182, 0.0364, 0.0364, 0.0364, 0.0727]), ([0.0417, 0.0113, 0.0673, 0.0748, 0.0253], [0.0182, 0.0, -0.0182, 0.0, 0.0]), ([0.0303, -0.0005, 0.0572, 0.0639, 0.0138], [0.0189, 0.0377, 0.0, 0.0189, 0.0377])]\n",
      "Epoch 55 [0.029948]: 0.114305\n",
      "Epoch 60 [0.028480]: 0.0295726\n",
      "Epoch 65 [0.027084]: 0.060324\n",
      "Epoch 70 [0.025757]: 0.0303507\n",
      "Epoch 75 [0.024495]: 0.0314448\n",
      "Epoch 80 [0.023294]: 0.018436\n",
      "Epoch 85 [0.022152]: 0.0663752\n",
      "Epoch 90 [0.021067]: 0.0135205\n",
      "Epoch 95 [0.020034]: 0.0709529\n",
      "Epoch 100 [0.019052]: 0.0169788\n",
      "Predictions: [([-0.0124, -0.1331, -0.1072, 0.1166, 0.1037], [0.0161, 0.0, -0.0161, -0.0161, 0.0]), ([0.0637, 0.0747, 0.0847, 0.129, 0.1127], [0.0727, 0.0909, 0.1091, 0.1091, 0.1273]), ([-0.0465, -0.0376, -0.0305, 0.0112, -0.001], [0.0182, 0.0364, 0.0364, 0.0364, 0.0727]), ([-0.0151, -0.0068, -0.0006, 0.043, 0.0291], [0.0182, 0.0, -0.0182, 0.0, 0.0]), ([-0.0028, 0.0071, 0.0149, 0.059, 0.0438], [0.0189, 0.0377, 0.0, 0.0189, 0.0377])]\n",
      "Epoch 105 [0.018119]: 0.0136485\n",
      "Epoch 110 [0.017231]: 0.0144659\n",
      "Epoch 115 [0.016386]: 0.0354998\n",
      "Epoch 120 [0.015583]: 0.0148369\n",
      "Epoch 125 [0.014819]: 0.0148515\n",
      "Epoch 130 [0.014093]: 0.00904694\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "final_prediction = []\n",
    "final_loss = None\n",
    "\n",
    "graph_name = \"All_lr%.2f_lr_decay%.3f_lstm%d_step%d_input%d_batch%d_epoch%d_symbol_embed%d\" % (\n",
    "    config.init_learning_rate, config.learning_rate_decay, \n",
    "    config.lstm_size, config.num_steps, \n",
    "    config.input_size, config.batch_size, config.max_epoch,\n",
    "    config.embedding_size)\n",
    "\n",
    "print \"Graph Name:\", graph_name\n",
    "\n",
    "log_dir = '_logs/' + graph_name\n",
    "model_dir = \"_models/\" + graph_name\n",
    "\n",
    "_mkdir_if_not_exist(log_dir)\n",
    "_mkdir_if_not_exist(model_dir)\n",
    "\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w') as fout:\n",
    "    print >> fout, \"symbol\" + \"\\t\" + \"sector\"\n",
    "    for stock_sym in sorted(symbol_to_index, key=lambda x: symbol_to_index[x]):\n",
    "        print >> fout, stock_sym + \"\\t\" + stock_data_by_sym[stock_sym].stock_sector\n",
    "\n",
    "with tf.Session(graph=lstm_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "\n",
    "    ######################################################\n",
    "    ### Set up embedding visualization\n",
    "    # Format: tensorflow/tensorboard/plugins/projector/projector_config.proto\n",
    "    projector_config = projector.ProjectorConfig()\n",
    "\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    added_embedding = projector_config.embeddings.add()\n",
    "    added_embedding.tensor_name = embedding_matrix.name\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    added_embedding.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    \n",
    "    # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "    # read this file during startup.\n",
    "    projector.visualize_embeddings(summary_writer, projector_config)\n",
    "    ######################################################\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    test_data_feed = {\n",
    "        inputs: merged_test_X,\n",
    "        targets: merged_test_y,\n",
    "        learning_rate: 0.0,\n",
    "        stock_labels: merged_stock_symbol_indices,\n",
    "    }\n",
    "\n",
    "    for epoch_step in range(config.max_epoch):\n",
    "        current_lr = learning_rates_to_use[epoch_step]\n",
    "        \n",
    "        for symbol, stock_dataset in stock_data_by_sym.iteritems():\n",
    "            stock_sector_index = sector_to_index[stock_dataset.stock_sector]\n",
    "            stock_symbol_index = symbol_to_index[stock_dataset.stock_sym]\n",
    "            \n",
    "            for batch_X, batch_y in stock_dataset.generate_one_epoch(config.batch_size):\n",
    "                train_data_feed = {\n",
    "                    inputs: batch_X, \n",
    "                    targets: batch_y, \n",
    "                    learning_rate: current_lr,\n",
    "                    stock_labels: np.array([stock_symbol_index] * len(batch_X))\n",
    "                }\n",
    "                train_loss, _ = sess.run([loss, minimize], train_data_feed)\n",
    "        \n",
    "        if epoch_step % 5 == 0:\n",
    "            test_loss, _pred, _summary = sess.run([loss, prediction, merged_summary], test_data_feed)\n",
    "            assert len(_pred) == len(merged_test_y)\n",
    "            print \"Epoch %d [%f]:\" % (epoch_step, current_lr), test_loss\n",
    "            if epoch_step % 50 == 0:\n",
    "                print \"Predictions:\", [(\n",
    "                    map(lambda x: round(x, 4), _pred[-j]), \n",
    "                    map(lambda x: round(x, 4), merged_test_y[-j])\n",
    "                ) for j in range(5)]\n",
    "\n",
    "            # Save a checkpoint\n",
    "            saver.save(sess, os.path.join(log_dir, \"model.ckpt\"), epoch_step)\n",
    "            summary_writer.add_summary(_summary, global_step=epoch_step)\n",
    "    \n",
    "    print \"Final Results:\"\n",
    "    final_prediction, final_loss = sess.run([prediction, loss], test_data_feed)\n",
    "    print final_prediction, final_loss\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    saver.save(sess, \"{0}/stock_rnn_model_{0}.ckpt\".format(model_dir), global_step=epoch_step)\n",
    "\n",
    "\n",
    "with open(\"final_predictions.{}.json\".format(graph_name), 'w') as fout:\n",
    "    fout.write(json.dumps(final_prediction.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load a trained model\n",
    "import tensorflow as tf\n",
    "\n",
    "test_prediction = None\n",
    "test_loss = None\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    # First let's load meta graph and restore weights\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('_models/stock_rnn_model_{}.ckpt-499.meta'.format(name)))\n",
    "    saver = tf.train.import_meta_graph('_models/stock_rnn_model_{}.ckpt-499.meta'.format(name))\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('_models/.'))\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    print graph.get_operations\n",
    "    print sess.graph\n",
    "\n",
    "    test_feed_dict = {\n",
    "        graph.get_tensor_by_name('inputs:0'): SP500_dataset.test_X, \n",
    "        graph.get_tensor_by_name('targets:0'): SP500_dataset.test_y, \n",
    "        graph.get_tensor_by_name('learning_rate:0'): 0.0\n",
    "    }\n",
    "    \n",
    "    ops = graph.get_collection('ops_to_restore')\n",
    "    print ops\n",
    "\n",
    "    prediction = graph.get_tensor_by_name('output_layer/add:0')\n",
    "    loss = graph.get_tensor_by_name('train/loss_mse:0')\n",
    "\n",
    "    test_prediction, test_loss = sess.run([prediction, loss], test_feed_dict)\n",
    "    print test_prediction, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01612903  0.         -0.01612903 -0.01612903  0.        ]\n",
      " [ 0.          0.03125     0.03125     0.046875    0.03125   ]\n",
      " [ 0.06451613  0.01612903  0.          0.01612903 -0.01612903]\n",
      " [-0.1969697  -0.18181818 -0.18181818 -0.16666667 -0.16666667]\n",
      " [-0.09836066 -0.14754098 -0.1147541  -0.09836066 -0.1147541 ]\n",
      " [ 0.          0.01818182  0.          0.01818182  0.01818182]\n",
      " [ 0.03703704  0.01851852  0.07407407  0.09259259  0.09259259]\n",
      " [ 0.05357143  0.08928571  0.07142857  0.08928571  0.05357143]\n",
      " [-0.01694915 -0.03389831 -0.03389831  0.05084746  0.08474576]\n",
      " [ 0.08474576  0.08474576  0.08474576  0.05084746  0.05084746]]\n",
      "[34 34 34 34 34 34 34 34 34 34]\n",
      "[[-0.02814335 -0.02744995 -0.02656104 -0.02613362 -0.02431634]\n",
      " [ 0.00884791  0.00895659  0.00982992  0.00984628  0.01205399]\n",
      " [ 0.0466904   0.04632494  0.04728033  0.04821865  0.04977461]\n",
      " [-0.03051527 -0.02972901 -0.02864716 -0.02775948 -0.02567241]\n",
      " [-0.06001961 -0.05907817 -0.05899962 -0.05880501 -0.05733833]\n",
      " [-0.02295013 -0.02232821 -0.02129837 -0.02055316 -0.01841485]\n",
      " [-0.00425968 -0.00442606 -0.00271164 -0.00253609 -0.00077598]\n",
      " [ 0.02768748  0.02770022  0.0287015   0.02938667  0.03179573]\n",
      " [ 0.00782637  0.00867315  0.00922898  0.00910058  0.01157795]\n",
      " [-0.01615319 -0.0165002  -0.01511858 -0.0145394  -0.01409296]]\n"
     ]
    }
   ],
   "source": [
    "print merged_test_y[:10]\n",
    "print merged_stock_symbol_indices[:10]\n",
    "print final_prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_to_final_prediction = {}\n",
    "for sym_label, sym_index in symbol_to_index.iteritems():\n",
    "    target_indices = np.array([\n",
    "        _idx for _idx, _sym_index in enumerate(merged_stock_symbol_indices) \n",
    "        if _sym_index == sym_index])\n",
    "    sym_to_final_prediction[sym_label] = final_prediction[target_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _flatten(seq):\n",
    "    return [x for y in seq for x in y]\n",
    "\n",
    "def _denormalize(raw_seq, test_y):\n",
    "    base_seq_index = len(SP500_dataset.train_X)\n",
    "    print \"start_point:\", raw_seq[base_seq_index]\n",
    "    denorm_test_y = []\n",
    "    for seq in test_y:\n",
    "        denorm_seq = (np.array(seq) + 1.0) * raw_seq[base_seq_index]\n",
    "        base_seq_index += 1\n",
    "        denorm_test_y += list(denorm_seq)\n",
    "    return denorm_test_y\n",
    "\n",
    "def plot_test(truths, preds, xlab=None, ylab=None, title=None):\n",
    "    days = range(len(truths))\n",
    "    print title, 'Total num. of days:', len(days)\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(days, truths, label='truth')\n",
    "    plt.plot(days, preds, label='pred')\n",
    "    plt.legend()\n",
    "    plt.xlabel(xlab or \"day\")\n",
    "    plt.ylabel(ylab or \"normalized price\")\n",
    "    plt.grid(ls='--')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "selected_symbols = random.sample(sym_to_final_prediction.keys(), 5)\n",
    "\n",
    "for sym in selected_symbols:\n",
    "    sym_final_prediction = sym_to_final_prediction[sym]\n",
    "    sym_test_y = stock_data_by_sym[sym].test_y\n",
    "\n",
    "    plot_test(_flatten(sym_test_y)[:200], _flatten(sym_final_prediction)[:200], \n",
    "              xlab='first 200 days in test data', title=sym)\n",
    "    plot_test(_flatten(sym_test_y)[-200:], _flatten(sym_final_prediction)[-200:], \n",
    "              xlab='last 200 days in test data', title=sym)\n",
    "    plot_test(_flatten(sym_test_y), _flatten(sym_final_prediction), title=sym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plotting_date = pd.read_csv(\"_data/SP500.csv\").tail(1000)\n",
    "plotting_date['Date'] = pd.to_datetime(plotting_date['Date'])\n",
    "print plotting_date.head()\n",
    "plt.plot_date(plotting_date['Date'], plotting_date['Close'], '-', color='#e01f1f')\n",
    "plt.ylabel('Daily S&P 500 Close Price')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "plt.grid(ls=':', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One step: input_size=3, num_steps=2\n",
    "original [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "input_size=3 [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "len(seq) = 4\n",
    "apply num_steps=2\n",
    "X = [\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[4, 5, 6], [7, 8, 9]],\n",
    "]\n",
    "y = [\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12],\n",
    "]\n",
    "\n",
    "    # split into groups of num_steps\n",
    "    X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "    y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
